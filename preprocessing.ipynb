{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/quytien/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quytien/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/quytien/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from underthesea import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from vncorenlp import VnCoreNLP\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "461c0bf7-db4b-4096-88ac-3f2e6da37d1c",
   "metadata": {},
   "source": [
    "### 1. Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, d·∫•u c√¢u, emoji \n",
    "#### *- Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng xo√° c√°c k√Ω t·ª± ti·∫øng Vi·ªát (Unicode property): **r\"[^\\w\\s√Ä-·ªπ]\"*** <br /> *- Lo·∫°i b·ªè to√†n b·ªô k√Ω t·ª± ƒë·∫∑c bi·ªát, ch·ªâ gi·ªØ l·∫°i c√°c k√Ω t·ª± ASCII (ti·∫øng Anh kh√¥ng d·∫•u): **r\"[^a-zA-Z0-9]\"***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5367cc99-6680-4177-b26b-7395f1f2a242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch·ªâ c·∫ßn ki√™n nh·∫´n r·ªìi m·ªçi th·ª© s·∫Ω ƒë·∫øn\n"
     ]
    }
   ],
   "source": [
    "txt = \"Ch·ªâ c·∫ß$n ki√™n nh@·∫´n r·ªìi m·ªçi t.h·ª© s·∫Ω ƒë·∫øn.\"\n",
    "\n",
    "new_string = re.sub(r\"[^\\w\\s√Ä-·ªπ]\",\"\",txt)\n",
    "print(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800a17c-65ae-4c1c-8638-a9f954d603be",
   "metadata": {},
   "source": [
    "#### Lo·∫°i b·ªè d·∫•u c√¢u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94dfdf50-c859-4bcd-8187-7b8f7a6a46db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cu·ªôc s·ªëng kh√¥ng ph·∫£i l√∫c n√†o c≈©ng nh∆∞ m√¨nh mu·ªën\n"
     ]
    }
   ],
   "source": [
    "text = \"Cu·ªôc, s·ªëng kh√¥ng? ph·∫£i l√∫c n√†o; c≈©ng nh∆∞ m√¨nh: mu·ªën!\"\n",
    "\n",
    "new_string = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "print(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e495c-39fa-45cb-b09c-07d53382b75b",
   "metadata": {},
   "source": [
    "#### Lo·∫°i b·ªè emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65a909f3-f878-403e-99bf-ad87b2067dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H√¥m nay tr·ªùi ƒë·∫πp qu√°! ƒêi ch∆°i th√¥i \n"
     ]
    }
   ],
   "source": [
    "text = \"H√¥m nay tr·ªùi ƒë·∫πp qu√°üòçüåû! ƒêi ch∆°i th√¥i üèñÔ∏èüöó\"\n",
    "\n",
    "new_string = emoji.replace_emoji(text, replace='')\n",
    "print(new_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22390dfb-b8cb-44a9-85dc-4f831a64d1d0",
   "metadata": {},
   "source": [
    "### 2.\tChu·∫©n h√≥a v√† t√°ch t·ª´ ti·∫øng Vi·ªát\n",
    "#### T√°ch t·ª´ s·ª≠ d·ª•ng th∆∞ vi·ªán underthesea (h·ªó tr·ª£ version 10, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21303f6d-2926-47f5-a416-8df91603d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V√† em lu√¥n tin sau c∆°n m∆∞a c·∫ßu v·ªìng s·∫Ω l·∫•p_l√°nh .\n"
     ]
    }
   ],
   "source": [
    "txt = \"V√† em lu√¥n tin sau c∆°n m∆∞a c·∫ßu v·ªìng s·∫Ω l·∫•p l√°nh.\"\n",
    "\n",
    "tokens = word_tokenize(txt, format=\"text\")\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa062532-e3c7-4220-ba25-55193e715bd9",
   "metadata": {},
   "source": [
    "#### T√°ch t·ª´ s·ª≠ d·ª•ng th∆∞ vi·ªán VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af48cc6-1734-4f73-9592-84ee8ccb4df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['V√†', 'em', 'lu√¥n', 'tin', 'sau', 'c∆°n', 'm∆∞a', 'c·∫ßu_v·ªìng', 's·∫Ω', 'l·∫•p_l√°nh', '.']]\n"
     ]
    }
   ],
   "source": [
    "vncorenlp = VnCoreNLP(\n",
    "    \"/Users/quytien/VnCoreNLP/VnCoreNLP-1.2.jar\", \n",
    "    annotators=\"wseg\", \n",
    "    max_heap_size='-Xmx2g'\n",
    ")\n",
    "res = vncorenlp.tokenize(txt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c8a29-c9b3-4684-8828-794ea309e284",
   "metadata": {},
   "source": [
    "#### Chu·∫©n ho√°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118473e9-124a-43e7-bb5d-7f6ff3ad4f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v√† em lu√¥n tin sau c∆°n m∆∞a c·∫ßu v·ªìng s·∫Ω l·∫•p l√°nh.\n"
     ]
    }
   ],
   "source": [
    "txt_1 = \"V√†               em lu√¥n tin sau c∆°n m∆∞a c·∫ßu v·ªìng s·∫Ω l·∫•p l√°nh.\"\n",
    "# Chu·∫©n h√≥a Unicode\n",
    "txt_1 = unicodedata.normalize('NFC', txt_1)\n",
    "\n",
    "# Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng\n",
    "txt_1 = txt_1.lower()\n",
    "\n",
    "# Lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a\n",
    "txt_1 = re.sub(r\"\\s+\", \" \", txt_1).strip()\n",
    "print(txt_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2999cb6f-fa4f-4422-bf92-3432d0034597",
   "metadata": {},
   "source": [
    "### 3.\tLo·∫°i b·ªè stopwords v√† stemming/lemmatization\n",
    "#### Lo·∫°i b·ªè stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21966245-b25d-41d6-a582-b61f38a2ebde",
   "metadata": {},
   "source": [
    "Th∆∞ vi·ªán nltk ch∆∞a h·ªó tr·ª£ ti·∫øng Vi·ªát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac2f157c-b50d-413d-ad2f-dfb400e8d6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = stopwords.words('english')\n",
    "print(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "020945f4-a362-4387-b939-e3b20995f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vietnamese_stopwords(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return set(line.strip().lower() for line in f if line.strip())\n",
    "\n",
    "vietnamese_stopwords = load_vietnamese_stopwords(\"vietnamese-stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3da7e9a5-615c-45d9-817b-bb9762cffeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ta', 'b·∫Øt_g·∫∑p', 'kh·∫Øp', 'S√†i_G√≤n', 'y√™u']\n"
     ]
    }
   ],
   "source": [
    "text_stopwords = \"Ta t·ª´ng b·∫Øt g·∫∑p nhau ·ªü kh·∫Øp S√†i G√≤n ch·∫Øc l√† l√∫c c√≤n y√™u d√π mu·ªën tr√°nh c≈©ng kh√≥\"\n",
    "# T√°ch t·ª´\n",
    "tokens_stopwords = word_tokenize(text_stopwords, format=\"text\").split()\n",
    "# L·ªçc stopwords\n",
    "filtered_tokens = [word for word in tokens_stopwords if word.lower() not in vietnamese_stopwords]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76497541-e41f-4f93-b1ac-d96710277184",
   "metadata": {},
   "source": [
    "#### Stemming/ Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f27433-8c07-4ed5-bc69-7bb5adf41275",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "623776db-2e43-4ca2-ac9c-698e23897396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "input_str = \"There are several types of stemming algorithms.\"\n",
    "input_str = nltk.word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7725ad-9007-42e0-9099-a09e1fb0d509",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77226f14-5672-416f-ba90-605b3ca8195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "had\n",
      "done\n",
      "language\n",
      "city\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "input_str = \"been had done languages cities mice\"\n",
    "input_str = nltk.word_tokenize(input_str)\n",
    "\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "713bbd88-695b-4493-8eb1-0757dd08152b",
   "metadata": {},
   "source": [
    "### 4.\tX√¢y d·ª±ng b·ªô t·ª´ ƒëi·ªÉn n-gram t·ª´ vƒÉn b·∫£n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a749a0-cd69-44b7-9850-65de40d5a77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'go': 4, ',': 3, '!': 2, 'don‚Äôt': 1, 'stop': 1}\n"
     ]
    }
   ],
   "source": [
    "def build_ngram(text, n=1):\n",
    "    text\n",
    "    tokens = word_tokenize(text.lower(), format=\"text\").split()\n",
    "    ngrams = defaultdict(int)\n",
    "    for i in range (len(tokens) - n + 1):\n",
    "        ngram = ' '.join(tokens[i:i+n])\n",
    "        ngrams[ngram] += 1\n",
    "    return dict(ngrams)\n",
    "\n",
    "text = \"Go, go, go, go! Don‚Äôt stop!\"\n",
    "bigram_dict = build_ngram(text, 1)\n",
    "\n",
    "print(bigram_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
